{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requisitos Previos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Flask-CORS\n",
    "!pip install neattext\n",
    "!pip install emoji\n",
    "!pip install scikit-learn==1.4.2 \n",
    "!pip install selenium\n",
    "\n",
    "!pip install joblib\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "# from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import request, jsonify, Flask\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion axuiliar para saber si estoy en el collab y usar su path o el del proyecto de github\n",
    "def is_running_on_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "    \n",
    "from enum import Enum\n",
    "# Funcion auxiliar para luego entrenar varios modelos con una sola ejecucion\n",
    "class Modelos(Enum):\n",
    "    LOGISTIC_REGRESSION = 'logistic_regression'\n",
    "    DECISION_TREE = 'decision_tree'\n",
    "    MULTINOMIAL = 'multinomial'\n",
    "    BERNOULLI = 'bernoulli'\n",
    "    GAUSIAN = 'gausian'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funcion procesador texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neattext.functions as nfx\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import emoji\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def penn2morphy(penntag):\n",
    "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
    "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "                  'VB':'v', 'RB':'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n'\n",
    "\n",
    "\n",
    "stopwords_en = stopwords.words('english')\n",
    "stopwords_en = set(stopwords_en).union(set(punctuation))\n",
    "\n",
    "my_custom_stopwords = {'’', \"n't\", \"'m\", \"'s\", \"'ve\", '...', 'ca', \"''\", '``', '\\u200d', 'im', 'na', \"'ll\", '..', 'u', \"'re\", \"'d\", '--', '”', '“', '\\u200f\\u200f\\u200e', '....', 'ㅤ','\\u200e\\u200f\\u200f\\u200e', 'x200b', 'ive', '.-', '\\u200e', '‘'}\n",
    "\n",
    "stopwords_en = stopwords_en.union(my_custom_stopwords)\n",
    "\n",
    "\n",
    "def preprocessing_function(text):\n",
    "    words = []\n",
    "\n",
    "    for word, tag in pos_tag(word_tokenize(nfx.clean_text(text))):\n",
    "        word_lemmatized = wnl.lemmatize(word.lower(), pos=penn2morphy(tag))\n",
    "\n",
    "        if '\\u200b' in word_lemmatized:\n",
    "            continue\n",
    "\n",
    "        if word_lemmatized not in stopwords_en and not word_lemmatized.isdigit() and not emoji.purely_emoji(word_lemmatized):\n",
    "            words.append(word_lemmatized)\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFICAR ESTOS PARAMETROS PARA LA CARGA\n",
    "# --------------------------------------------------------\n",
    "nombre_modelo_prev_entrenado = Modelos.LOGISTIC_REGRESSION.value\n",
    "# usar formato '25k' para 25.000 filas ejemplo\n",
    "cant_prev_entrenada = '50k'\n",
    "\n",
    "path_base_modelo_generado = '/content/' if is_running_on_colab() else '.\\\\tentativa_suicidio\\\\entrenados\\\\'\n",
    "path_modelo_generado = path_base_modelo_generado + nombre_modelo_prev_entrenado + '_' + cant_prev_entrenada\n",
    "# --------------------------------------------------------\n",
    "\n",
    "model = joblib.load(path_modelo_generado + '_model.pkl')\n",
    "vect = joblib.load(path_modelo_generado + '_vector.pkl')\n",
    "\n",
    "print(type(vect))\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funcion predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translator = Translator()\n",
    "\n",
    "def get_tentativa_suicidio(text_input, english_text=False):\n",
    "    texto_a_analizar = text_input #if english_text else translator.translate(text_input, dest='en').text\n",
    "        \n",
    "    texto_preprocesado = ' '.join(preprocessing_function(texto_a_analizar))\n",
    "    texto_vectorizado = vect.transform([texto_preprocesado])\n",
    "\n",
    "    return bool(model.predict(texto_vectorizado)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funcion obtener texto de url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import re\n",
    "\n",
    "def get_texto(url):\n",
    "    options = webdriver.FirefoxOptions()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    driver.get(url)\n",
    "\n",
    "    texto = \"\"\n",
    "    hay_read_more = False\n",
    "    time.sleep(1)\n",
    "\n",
    "    try:\n",
    "        buttons = driver.find_elements(By.XPATH, \"//button[contains(@id, '-read-more-button')]\") \n",
    "        for button in buttons:\n",
    "            if re.search(r\"-read-more-button$\", button.get_attribute(\"id\")):\n",
    "                if button.is_displayed():\n",
    "                    hay_read_more = True\n",
    "                    button.click()\n",
    "                    break\n",
    "        if(hay_read_more):\n",
    "            button_id = button.get_attribute(\"id\")\n",
    "            div_id_pattern = re.sub(r\"-read-more-button$\", \"-post-rtjson-content\", button_id)\n",
    "            texto_element = driver.find_element(By.XPATH, f\"//div[contains(@id, '{div_id_pattern}')]\")\n",
    "        else:\n",
    "            parent_div = driver.find_element(By.XPATH, \"//div[@class='text-neutral-content']\")\n",
    "            texto_element = parent_div.find_element(By.XPATH, \".//div[contains(@id, '-post-rtjson-content')]\")\n",
    "            \n",
    "        texto = texto_element.text\n",
    "    finally:\n",
    "        driver.close()\n",
    "\n",
    "\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask_cors import CORS\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "CORS(app)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return \"<p>API para predecir tentativas de suicidio en textos.</p>\"\n",
    "\n",
    "@app.route('/test', methods=['GET'])\n",
    "def test():\n",
    "    return jsonify({\"message\": \"Funciona\"})\n",
    "\n",
    "\n",
    "@app.route('/text_prediction', methods=['GET'])\n",
    "def procesar_texto():\n",
    "    text_input = request.args.get('text')\n",
    "    if text_input:\n",
    "        prediction = get_tentativa_suicidio(text_input)\n",
    "        return jsonify({\"prediction\": prediction})\n",
    "    else:\n",
    "        return jsonify({\"error\": \"No se proporció el texto en la solicitud\"})\n",
    "    \n",
    "@app.route(\"/url_predition\", methods=[\"POST\"])\n",
    "def procesar_url():\n",
    "    data = request.json\n",
    "    url = data.get(\"url\")\n",
    "    if url:\n",
    "        print(\"URL recibida: \", url)\n",
    "        texto = get_texto(url)\n",
    "        print(\"Texto obtenido: \", texto)\n",
    "        prediction = get_tentativa_suicidio(texto)\n",
    "        print(\"Predicción: \", prediction)\n",
    "        return jsonify({\"prediction\": prediction})\n",
    "    else:\n",
    "        return jsonify({\"error\": \"No se proporcionó la URL en la solicitud\"})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
