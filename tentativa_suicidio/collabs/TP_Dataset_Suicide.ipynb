{"cells":[{"cell_type":"markdown","metadata":{"id":"xwDzlfPptSwb"},"source":["# Librerias"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4197,"status":"ok","timestamp":1714742773677,"user":{"displayName":"Victor Povoli Olivera","userId":"04383995252204734605"},"user_tz":180},"id":"8Q8yzQs0oM5z","outputId":"5d59bed7-d10c-476d-cc89-b3e4754a5237"},"outputs":[],"source":["# trabajar con datos tabulares\n","import pandas as pd\n","# nlp\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","nltk.download('word2vec_sample')\n","# guardado del modelo entranado\n","import pickle\n","import joblib\n","# eliminar warning del replace\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","# traducir\n","from googletrans import Translator\n","# emojis\n","import emoji"]},{"cell_type":"markdown","metadata":{},"source":["# Pasos"]},{"cell_type":"markdown","metadata":{"id":"SxPDjNHatZYY"},"source":["## Importacion dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":280},"executionInfo":{"elapsed":278,"status":"error","timestamp":1714742937245,"user":{"displayName":"Victor Povoli Olivera","userId":"04383995252204734605"},"user_tz":180},"id":"c62w5_G2oVUg","outputId":"1d0967bc-7010-40aa-ab60-0f2c9449f060"},"outputs":[],"source":["# Ruta del archivo CSV\n","file_path = '..\\\\datasets\\\\Suicide_Detection.csv'\n","\n","# Leer el archivo CSV en un DataFrame de Pandas, \n","# dataframe = pd.read_csv(file_path)\n","# si quiero limitar la cantidad a importar\n","dataframe = pd.read_csv(file_path, nrows=25000)\n","\n","# Este caso puntual el csv la primera columna es el indice que no nos interesa, si quiero eliminarla por el nombre que le asigna pandas\n","# dataframe = dataframe.drop('Unnamed: 0', axis=1)\n","# o eliminarla por la posicion\n","dataframe = dataframe.drop(dataframe.columns[0], axis=1)\n","\n","# Paso a booleano la clasificacion\n","dataframe['class'] = dataframe['class'].replace({\"suicide\": True, \"non-suicide\": False})\n","\n","print(dataframe)"]},{"cell_type":"markdown","metadata":{"id":"pyOZvVNZ0Cne"},"source":["## Entrenamiento"]},{"cell_type":"markdown","metadata":{},"source":["### Funcion analizadora"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from nltk import pos_tag, word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","from string import punctuation\n","\n","# lematization\n","wnl = WordNetLemmatizer()\n","\n","def penn2morphy(penntag):\n","    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n","    morphy_tag = {'NN':'n', 'JJ':'a',\n","                  'VB':'v', 'RB':'r'}\n","    try:\n","        return morphy_tag[penntag[:2]]\n","    except:\n","        return 'n'\n","\n","\n","# Agrego a stopwords signos de puntuacion y emojis\n","stopwords_en = stopwords.words('english')\n","stopwords_en = set(stopwords_en).union(set(punctuation))\n","# stopwords_en = set(stopwords_en).union(set(emoji.UNICODE_EMOJI['en']))\n","\n","# Defino la funcion\n","def preprocessing_function(text):\n","    words = []\n","\n","    for word, tag in pos_tag(word_tokenize(text)):\n","        word_lemmatized = wnl.lemmatize(word.lower(), pos=penn2morphy(tag))\n","\n","        if word_lemmatized not in stopwords_en and not word_lemmatized.isdigit() and not emoji.purely_emoji(word_lemmatized):\n","            print(word_lemmatized)\n","            words.append(word_lemmatized)\n","\n","    return words"]},{"cell_type":"markdown","metadata":{},"source":["### Entrenamiento con bolsa de palabras"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"91LdWl8I0Eth"},"outputs":[],"source":["# Hacemos un split de sets de train y test\n","from sklearn.model_selection import train_test_split\n","\n","\n","X_train, X_test, y_train, y_test = train_test_split(dataframe[\"text\"],\n","                                                    dataframe[\"class\"],\n","                                                    test_size=0.15, random_state=0,\n","                                                    stratify=dataframe[\"class\"])\n","\n","len(y_train), len(y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cBijtjrz0M8A"},"outputs":[],"source":["# Sklearn tiene un objeto llamado CountVectorizer que nos permite pasarle un \"analyzer\"\n","# El \"analyzer\" toma el texto que le pasamos y devuelve una lista de palabras a contar.\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","count_vectorizer = CountVectorizer(analyzer=preprocessing_function)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Entrenamos nuestro CountVectorizer en el training set and transformamos ambos datasets\n","X_train_vectorized = count_vectorizer.fit_transform(X_train)\n","X_test_vectorized = count_vectorizer.transform(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["## Testing"]},{"cell_type":"markdown","metadata":{},"source":["### Funcion test"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Vamos a definir nuestra funcion de test y graficar nuestra confusion matrix.\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","def test(clf):\n","  clf.fit(X_train_vectorized.toarray(), y_train)\n","  y_pred = clf.predict(X_test_vectorized.toarray())\n","\n","  print(f\"accuracy: {accuracy_score(y_true=y_test, y_pred=y_pred)}\")\n","\n","  return ConfusionMatrixDisplay.from_estimator(\n","      clf, X_test_vectorized.toarray(), y_test,  xticks_rotation=\"vertical\"\n","  )"]},{"cell_type":"markdown","metadata":{},"source":["### Decision Tree"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","clf = DecisionTreeClassifier()\n","test(clf)"]},{"cell_type":"markdown","metadata":{},"source":["### Guardado"]},{"cell_type":"markdown","metadata":{},"source":["Una version"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import joblib\n","\n","joblib.dump(clf, '..\\\\entrenados\\\\decision_tree\\\\joblib\\\\decision_tree_model_25k.pkl')\n","joblib.dump(count_vectorizer, '..\\\\entrenados\\\\decision_tree\\\\joblib\\\\decision_tree_vector_25k.pkl')"]},{"cell_type":"markdown","metadata":{},"source":["El que teniamos"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pickle.dump(clf, open('..\\\\entrenados\\\\decision_tree\\\\dump\\\\decision_tree_model_25k.pkl', 'wb'))\n","pickle.dump(count_vectorizer, open('..\\\\entrenados\\\\decision_tree\\\\dump\\\\decision_tree_vector_25k.pkl', 'wb'))"]},{"cell_type":"markdown","metadata":{},"source":["### Naive Bayes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.naive_bayes import GaussianNB\n","clf = GaussianNB()\n","test(clf)"]},{"cell_type":"markdown","metadata":{},"source":["### Random Forest Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","clf = RandomForestClassifier(max_depth=2, random_state=42)\n","test(clf)"]},{"cell_type":"markdown","metadata":{},"source":["### Logistic Regression"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","lg = LogisticRegression()\n","test(lg)"]},{"cell_type":"markdown","metadata":{"id":"-4Ubam3DGzhI"},"source":["## Casos de uso"]},{"cell_type":"markdown","metadata":{},"source":["### Si uso el que recien genere"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vect = count_vectorizer\n","model = clf"]},{"cell_type":"markdown","metadata":{},"source":["### Si uso uno ya entrenado"]},{"cell_type":"markdown","metadata":{},"source":["Una version"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["loaded_model = joblib.load('..\\\\entrenados\\\\decision_tree\\\\joblib\\\\decision_tree_model_25k.pkl')\n","loaded_count_vectorizer = joblib.load('..\\\\entrenados\\\\decision_tree\\\\joblib\\\\decision_tree_vector_25k.pkl')\n"]},{"cell_type":"markdown","metadata":{},"source":["El que teniamos"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["loaded_model = pickle.load(open('..\\\\entrenados\\\\decision_tree\\\\dump\\\\decision_tree_model_25k.pkl', 'rb'))\n","lodead_count_vectorizer = pickle.load(open('..\\\\entrenados\\\\decision_tree\\\\dump\\\\decision_tree_vector_25k.pkl', 'rb'))"]},{"cell_type":"markdown","metadata":{},"source":["Asigno a las variables lo cargado"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vect = loaded_count_vectorizer\n","model = loaded_model"]},{"cell_type":"markdown","metadata":{},"source":["### Usando un dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["file_path = '..\\\\datasets\\\\Suicide_Detection.csv'\n","\n","dataframeTest = pd.read_csv(file_path, skiprows=100004, nrows=1)\n","dataframeTest.columns = [\"borrar\",\"text\",\"class\"]\n","dataframeTest = dataframeTest.drop(\"borrar\", axis=1)\n","dataframeTest['class'] = dataframeTest['class'].replace({\"suicide\": True, \"non-suicide\": False})\n","\n","print(dataframeTest)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for index, row in dataframeTest.iterrows():\n","    texto = row['text']\n","    print(texto)\n","    \n","    translator = Translator()\n","    traduccion = translator.translate(texto, dest='es').text\n","    print(traduccion)\n","    \n","\n","    texto_preprocesado = preprocessing_function(texto)\n","    texto_preprocesado_str = ' '.join(texto_preprocesado)\n","\n","    texto_vectorizado = vect.transform([texto_preprocesado_str])\n","\n","    prediccion = model.predict(texto_vectorizado)\n","\n","    clase_real = row['class']\n","    resultado_prediccion = 'suicida' if prediccion else 'no suicida'\n","    resultado_real = 'suicida' if clase_real else 'no suicida'\n","    \n","    # Imprimir el resultado de la predicción y la clase real\n","    print(f\"Predicción: {resultado_prediccion}. Clase real: {resultado_real}\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["### Test manual"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["textos_prueba = [\n","                \"I want to jump from a bridge\",\n","                \"I want to suicide me\",\n","                 \"I hate my parents with all my heart\",\n","                 \"I hate all about this life\",\n","                 \"I cry every night\",\n","                 \"I don't know what is happen to me, but I don't want live anymore\"\n","                 ]\n","\n","translator = Translator()\n","\n","for texto in textos_prueba:\n","    traduccion = translator.translate(texto, dest='es').text\n","    print(traduccion)\n","\n","    texto_preprocesado = preprocessing_function(texto)\n","    texto_preprocesado_str = ' '.join(texto_preprocesado)\n","\n","    texto_vectorizado = vect.transform([texto_preprocesado_str])\n","\n","    prediccion = model.predict(texto_vectorizado)\n","\n","    print(f\"Predicción para el texto '{texto}': {'suicida' if prediccion else 'no suicida'}\\n\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["## Verificar ocurrencia de palabras"]},{"cell_type":"markdown","metadata":{},"source":["### Palabras mas comunes contando palabra por palabra"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Intuición principal en este tipo de tecnicas: Contar las ocurrencias de las palabras.\n","from collections import defaultdict, Counter\n","from tqdm import tqdm\n","\n","# Instanciamos un contador de python\n","word_counts = Counter()\n","\n","for row in tqdm(dataframe.iterrows(), total=len(dataframe)):\n","    text = row[1]\n","\n","    # Usamos la funcion implementada en pandas split() para separar palabras por espacios en blanco.\n","    for word in text.str.split()[0]:\n","        word_counts[word] += 1\n","\n","len(word_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_counts.most_common(25)"]},{"cell_type":"markdown","metadata":{},"source":["### Palabras mas comunes contando con tokenizador"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from nltk import word_tokenize\n","\n","word_counts = Counter()\n","\n","for row in tqdm(dataframe.iterrows(), total=len(dataframe)):\n","    text = row[1].iat[0]\n","    for word in word_tokenize(text):\n","        word_counts[word] += 1\n","\n","len(word_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_counts.most_common(25)"]},{"cell_type":"markdown","metadata":{},"source":["### Palabras mas comunes contando teniendo en cuenta stopwords"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from nltk.corpus import stopwords\n","\n","stopwords_en = stopwords.words('english')\n","\n","word_counts = Counter()\n","\n","for row in tqdm(dataframe.iterrows(), total=len(dataframe)):\n","    text = row[1].iat[0]\n","\n","    for word in word_tokenize(text):\n","        word_lowercase = word.lower()\n","\n","        if word_lowercase not in stopwords_en:\n","            word_counts[word_lowercase] += 1\n","\n","len(word_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_counts.most_common(10)"]},{"cell_type":"markdown","metadata":{},"source":["### Palabras mas comunes contando teniendo en cuenta signos de puntuacion"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from string import punctuation\n","\n","# Hacemos una union entre conjunto de caracteres de puntuacion nativos a nuestro conjunto de stopwords usando la operation union de sets de datos.\n","stopwords_en = set(stopwords_en).union(set(punctuation))\n","\n","word_counts = Counter()\n","\n","for row in tqdm(dataframe.iterrows(), total=len(dataframe)):\n","    text = row[1].iat[0]\n","\n","    for word in word_tokenize(text):\n","        word_lowercase = word.lower()\n","\n","        if word_lowercase not in stopwords_en:\n","            word_counts[word_lowercase] += 1\n","\n","len(word_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_counts.most_common(10)"]},{"cell_type":"markdown","metadata":{},"source":["### Stemming"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Usamos Stemming\n","from nltk.stem import PorterStemmer\n","\n","porter = PorterStemmer()\n","\n","word_counts = Counter()\n","\n","for row in tqdm(dataframe.iterrows(), total=len(dataframe)):\n","    text = row[1].iat[0]\n","\n","    for word in word_tokenize(text):\n","        word_lowercase = word.lower()\n","\n","        if word_lowercase not in stopwords_en:\n","            stemmed_word = porter.stem(word_lowercase)\n","            word_counts[stemmed_word] += 1\n","\n","len(word_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_counts.most_common(10)"]},{"cell_type":"markdown","metadata":{},"source":["### Lematization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Usemos Lemmatization:\n","from nltk import word_tokenize\n","from nltk import pos_tag\n","from nltk.stem import WordNetLemmatizer\n","\n","\n","wnl = WordNetLemmatizer()\n","\n","def penn2morphy(penntag):\n","    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n","    morphy_tag = {'NN':'n', 'JJ':'a',\n","                  'VB':'v', 'RB':'r'}\n","    try:\n","        return morphy_tag[penntag[:2]]\n","    except:\n","        return 'n'\n","\n","word_counts = Counter()\n","\n","for row in tqdm(dataframe.iterrows(), total=len(dataframe)):\n","    text = row[1].iat[0]\n","\n","    for word, tag in pos_tag(word_tokenize(text)):\n","        word_lemmatized = wnl.lemmatize(word.lower(), pos=penn2morphy(tag))\n","\n","        if word_lemmatized not in stopwords_en:\n","            word_counts[word_lemmatized] += 1\n","\n","len(word_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_counts.most_common(10)"]}],"metadata":{"colab":{"provenance":[{"file_id":"1Uo1bHOz9RJfRzCG9S_8o3b1egLTtfyIW","timestamp":1714694532835},{"file_id":"14rAtWC1QzcGMRBgdRjrogYipZ2_RwVbQ","timestamp":1714693907215},{"file_id":"1bdtNIkY_2x9V_MbWLOtI0Qc-Jqm0bBwe","timestamp":1714222939750},{"file_id":"1CZK1XIyCXl3c51Uo3uy50EQqfnAFY1e6","timestamp":1714079362679}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"}},"nbformat":4,"nbformat_minor":0}
