{"cells":[{"cell_type":"markdown","metadata":{"id":"xwDzlfPptSwb"},"source":["# Librerias y funciones auxiliares"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4197,"status":"ok","timestamp":1714742773677,"user":{"displayName":"Victor Povoli Olivera","userId":"04383995252204734605"},"user_tz":180},"id":"8Q8yzQs0oM5z","outputId":"5d59bed7-d10c-476d-cc89-b3e4754a5237"},"outputs":[],"source":["# Importar csv y tratar con el dataset\n","import pandas as pd\n","# NLP\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","nltk.download('word2vec_sample')\n","# Eliminar warning del replace\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","# Detectar emojis y filtrarlos\n","import emoji\n","# from enum import Enum\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n","# Graficar\n","import matplotlib.pyplot as plt\n","# Guardado e importacion\n","import joblib\n","# Traducir\n","from googletrans import Translator\n","# Progreso\n","from tqdm import tqdm\n","\n","\n","from enum import Enum\n","\n","\n","# Funcion axuiliar para saber si estoy en el collab y usar su path o el del proyecto de github\n","def is_running_on_colab():\n","    try:\n","        import google.colab\n","        return True\n","    except ImportError:\n","        return False\n","    \n","# Funcion auxiliar para luego entrenar varios modelos con una sola ejecucion\n","class Modelos(Enum):\n","    LOGISTIC_REGRESSION = 'logistic_regression'\n","    DECISION_TREE = 'decision_tree'\n","    MULTINOMIAL = 'multinomial'\n","    BERNOULLI = 'bernoulli'\n","    GAUSIAN = 'gausian'\n","\n","def select_model_to_train(model_name):\n","    models = {\n","        Modelos.LOGISTIC_REGRESSION.value: LogisticRegression(max_iter=1000),\n","        Modelos.DECISION_TREE.value : DecisionTreeClassifier(),\n","        Modelos.MULTINOMIAL.value : MultinomialNB(),\n","        Modelos.BERNOULLI.value: BernoulliNB(),\n","        Modelos.GAUSIAN.value: GaussianNB()\n","    }\n","    if model_name in models:\n","        return models[model_name]\n","    else:\n","        raise ValueError(f\"Modelo '{model_name}' no válido\")\n","    \n","\n","def show_dataset_info(df, nombre_columna):\n","    print(\"Cantidad de registros:\")\n","    print(len(df))\n","\n","    print(\"Contenido:\")\n","    print(df.head())\n","\n","    conteo_valores = df[nombre_columna].value_counts()\n","    porcentaje_valores = conteo_valores / len(df) * 100\n","\n","\n","    for index, value in enumerate(conteo_valores):\n","        porcentaje = porcentaje_valores.iloc[index]\n","        plt.text(index, value + 0.5, '{} ({:.2f}%)'.format(value, porcentaje), ha='center')\n","        \n","\n","    conteo_valores.plot(kind='bar')\n","    plt.tight_layout()\n","    plt.xlabel('Valores Clasificaicon')\n","    plt.ylabel('Cant Valores')\n","    plt.title('Distribución de valores en la columna ' + nombre_columna)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Funcion procesador texto"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import neattext.functions as nfx\n","from nltk import pos_tag, word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","from string import punctuation\n","\n","wnl = WordNetLemmatizer()\n","\n","def penn2morphy(penntag):\n","    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n","    morphy_tag = {'NN':'n', 'JJ':'a',\n","                  'VB':'v', 'RB':'r'}\n","    try:\n","        return morphy_tag[penntag[:2]]\n","    except:\n","        return 'n'\n","\n","\n","stopwords_en = stopwords.words('english')\n","stopwords_en = set(stopwords_en).union(set(punctuation))\n","\n","my_custom_stopwords = {'’', \"n't\", \"'m\", \"'s\", \"'ve\", '...', 'ca', \"''\", '``', '\\u200d', 'im', 'na', \"'ll\", '..', 'u', \"'re\", \"'d\", '--', '”', '“', '\\u200f\\u200f\\u200e', '....', 'ㅤ','\\u200e\\u200f\\u200f\\u200e', 'x200b', 'ive', '.-', '\\u200e', '‘'}\n","\n","stopwords_en = stopwords_en.union(my_custom_stopwords)\n","\n","\n","def preprocessing_function(text):\n","    words = []\n","\n","    for word, tag in pos_tag(word_tokenize(nfx.clean_text(text))):\n","        word_lemmatized = wnl.lemmatize(word.lower(), pos=penn2morphy(tag))\n","\n","        if '\\u200b' in word_lemmatized:\n","            continue\n","\n","        if word_lemmatized not in stopwords_en and not word_lemmatized.isdigit() and not emoji.purely_emoji(word_lemmatized):\n","            words.append(word_lemmatized)\n","\n","    return words"]},{"cell_type":"markdown","metadata":{},"source":["# Entrenamiento"]},{"cell_type":"markdown","metadata":{},"source":["## Funcion entrenar modelo y ver metrica"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","import matplotlib.pyplot as plt\n","\n","def test(clf, nombreModelo):\n","  clf.fit(X_train_vectorized.toarray(), y_train)\n","  y_pred = clf.predict(X_test_vectorized.toarray())\n","\n","  accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n","\n","  disp = ConfusionMatrixDisplay.from_estimator(\n","        clf, X_test_vectorized.toarray(), y_test,  xticks_rotation=\"vertical\"\n","   )\n","  \n","  plt.title(nombreModelo + \" - Accuracy: {:.2f}\".format(accuracy))\n","\n","  # disp.plot()\n","  plt.show()\n","\n","  return disp"]},{"cell_type":"markdown","metadata":{"id":"SxPDjNHatZYY"},"source":["## Importacion dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":280},"executionInfo":{"elapsed":278,"status":"error","timestamp":1714742937245,"user":{"displayName":"Victor Povoli Olivera","userId":"04383995252204734605"},"user_tz":180},"id":"c62w5_G2oVUg","outputId":"1d0967bc-7010-40aa-ab60-0f2c9449f060"},"outputs":[],"source":["# MODIFICAR ESTOS PARAMETROS PARA LA IMPORTACION\n","# --------------------------------------------------------\n","usar_dataset_local = True\n","cant_importada = 500\n","path_base_dataset = '/content/' if is_running_on_colab() else '..\\\\datasets\\\\'\n","# PARAL LOCAL\n","nombre_archivo_dataset = 'Suicide_Detection.csv'\n","# PARA DESCARGAR\n","nombre_fuente_dataset = 'jquiros/suicide'\n","# AMBOS\n","columna_texto = 'text'\n","columna_clasificacion = 'class'\n","# --------------------------------------------------------\n","\n","if usar_dataset_local:\n","    path_dataset =  path_base_dataset + nombre_archivo_dataset\n","    dataframe = pd.read_csv(path_dataset, nrows=cant_importada)\n","else:\n","    from datasets import load_dataset\n","    dataset = load_dataset(nombre_fuente_dataset, \"default\")\n","    dataframe = pd.DataFrame({columna_texto: dataset['train'][columna_texto], \"class\": dataset['train'][columna_clasificacion]})\n","    dataframe = dataframe.head(cant_importada)\n","    dataset = None"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Colocar el nombre de la columna para ver la distribucion de datos\n","show_dataset_info(dataframe, columna_clasificacion)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Este caso puntual el csv local la primera columna es el indice que no nos interesa, si quiero eliminarla por el nombre que le asigna pandas\n","try:\n","    dataframe = dataframe.drop('Unnamed: 0', axis=1)\n","except:\n","    print(\"No se elimino columna Unnamed: 0\")\n","\n","# o eliminarla por la posicion\n","# dataframe = dataframe.drop(dataframe.columns[0], axis=1)\n","\n","# Paso a booleano la clasificacion\n","dataframe[columna_clasificacion] = dataframe[columna_clasificacion].replace({\"suicide\": True, \"non-suicide\": False})\n","dataframe[columna_clasificacion].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"pyOZvVNZ0Cne"},"source":["## Ajuste y guardado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cBijtjrz0M8A"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","\n","\n","X_train, X_test, y_train, y_test = train_test_split(dataframe[columna_texto],\n","                                                    dataframe[columna_clasificacion],\n","                                                    test_size=0.20, random_state=0,\n","                                                    stratify=dataframe[columna_clasificacion])\n","\n","len(y_train), len(y_test)\n","\n","\n","# ----------------------------------------------------------------------\n","#BoW con vectores binarios. Estos se usaban en tareas de analisis de sentimientos que no necesita saber la cantidad de veces que se repite una palabra sino su mera presencia.\n","count_vectorizer = CountVectorizer(analyzer=preprocessing_function, binary=True)\n","\n","# La idea es dado un texto hay que considerarlo una colección o bolsa (Bag) de palabras ignorando el orden y contexto.\n","# count_vectorizer = CountVectorizer(analyzer=preprocessing_function)\n","\n","#Term Frecuency - Inverse Document Frecuency trata este tema calculando la importancia de una palabra en base a las otras en el documento y en el corpus.\n","# count_vectorizer = TfidfVectorizer(analyzer=preprocessing_function)\n","\n","# ----------------------------------------------------------------------\n","\n","X_train_vectorized = count_vectorizer.fit_transform(X_train)\n","X_test_vectorized = count_vectorizer.transform(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# MODIFICAR ESTOS PARAMETROS PARA EL GUARDADO\n","# --------------------------------------------------------\n","# Colocar en true el modelo que se desea entrenar\n","modelos_disponibles = {\n","    Modelos.LOGISTIC_REGRESSION.value : False,\n","    Modelos.DECISION_TREE.value: False,\n","    Modelos.MULTINOMIAL.value: False,\n","    Modelos.BERNOULLI.value: False,\n","    Modelos.GAUSIAN.value: True,\n","}\n","path_base_modelo = '/content/' if is_running_on_colab() else '..\\\\entrenados\\\\'\n","# --------------------------------------------------------\n","\n","cant_entrenada_str = \"{:.0f}\".format(cant_importada/1000) + 'k'\n","\n","for nombreModelo, entrenar in modelos_disponibles.items():\n","    if entrenar:\n","        modelToTrain = select_model_to_train(nombreModelo)\n","        test(modelToTrain, nombreModelo)\n","\n","        path_modelo = path_base_modelo + nombreModelo + '_' + cant_entrenada_str\n","        joblib.dump(modelToTrain, path_modelo +  '_model.pkl')\n","        joblib.dump(count_vectorizer, path_modelo + '_vector.pkl')\n","        print(f\"Modelo {nombreModelo} guardado en \" + path_modelo)\n","        modelToTrain = None\n","    else:\n","        print(f\"Modelo {nombreModelo} no entrenado\")\n","    print(\"--------------------------------\")"]},{"cell_type":"markdown","metadata":{"id":"-4Ubam3DGzhI"},"source":["# Casos de uso"]},{"cell_type":"markdown","metadata":{},"source":["## Cargar modelo previamente generado"]},{"cell_type":"markdown","metadata":{},"source":["EJECUTAR PREVIAMENTE Funcion procesador texto"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# MODIFICAR ESTOS PARAMETROS PARA LA CARGA\n","# --------------------------------------------------------\n","nombre_modelo_prev_entrenado = Modelos.LOGISTIC_REGRESSION.value\n","# usar formato '25k' para 25.000 filas ejemplo\n","cant_prev_entrenada = '50k'\n","\n","path_base_modelo_generado = '/content/' if is_running_on_colab() else '..\\\\entrenados\\\\'\n","path_modelo_generado = path_base_modelo_generado + nombre_modelo_prev_entrenado + '_' + cant_prev_entrenada\n","# --------------------------------------------------------\n","\n","model = joblib.load(path_modelo_generado + '_model.pkl')\n","vect = joblib.load(path_modelo_generado + '_vector.pkl')\n","\n","print(type(vect))\n","print(type(model))"]},{"cell_type":"markdown","metadata":{},"source":["## Probar con dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# MODIFICAR ESTOS PARAMETROS PARA LA CARGA\n","# --------------------------------------------------------\n","# 0 si se quiere arrancar desde el principio, en este caso ya se incluye el nombre de las columnas del csv\n","nro_fila_arranque = 1\n","cant_a_probar = 5000\n","\n","path_base_dataset_test = '/content/' if is_running_on_colab() else '..\\\\datasets\\\\'\n","\n","nombre_archivo_dataset_test = 'Suicide_Detection.csv'\n","columna_texto_test = 'text'\n","columna_clasificacion_test = 'class'\n","clasificacion_true_test = 'suicide'\n","clasificacion_false_test = 'non-suicide'\n","# --------------------------------------------------------\n","\n","path_dataset =  path_base_dataset_test + nombre_archivo_dataset_test\n","dataframeTest = pd.read_csv(path_dataset, skiprows=nro_fila_arranque, nrows=cant_a_probar)\n","\n","columnas_renombradas = {columna_texto_test: 'text', columna_clasificacion_test: 'class'}\n","\n","if nro_fila_arranque != 0:\n","    dataframeColumns = pd.read_csv(path_dataset, nrows=1).columns\n","    dataframeTest.columns = dataframeColumns\n","\n","show_dataset_info(dataframeTest, columna_clasificacion_test)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","counter = 0\n","\n","for index, row in tqdm(dataframeTest.iterrows(), total=len(dataframeTest)):\n","    texto_test = row[columna_texto_test]\n","    clasificacion_test = True if row[columna_clasificacion_test] == clasificacion_true_test else False\n","\n","    texto_preprocesado = ' '.join(preprocessing_function(texto_test))\n","    texto_vectorizado = vect.transform([texto_preprocesado])\n","\n","    prediccion = model.predict(texto_vectorizado)[0]\n","\n","    if prediccion != clasificacion_test:\n","        counter = counter + 1\n","\n","print(\"La cantidad de casos donde no coincidio la prediccion con la clasificacion real del dataset: \" + str(counter) + \" - \" + str(\"{:.2f} %\".format(cant_a_probar / counter) if counter != 0 else \"no hubo errores\"))\n"]},{"cell_type":"markdown","metadata":{},"source":["## Probar con nuestro datos"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from googletrans import Translator\n","\n","textos_prueba = [\n","                # \"I want to jump from a bridge\",\n","                # \"I want to suicide me\",\n","                #  \"I hate my parents with all my heart\",\n","                #  \"I hate all about this life\",\n","                #  \"I cry every night\",\n","                 \"I don't know what is happen to me, but I don't want live anymore\",\n","                 \"Nose que me esta pasando, pero ya no quiero vivir mas\",\n","                 \"Non so cosa mi sta succedendo, ma non voglio più vivere.\"\n","                 ]\n","\n","translator = Translator()\n","\n","for texto in textos_prueba:\n","    texto_a_analizar = texto\n","    language = translator.detect(texto).lang\n","\n","    if(language != 'en'):\n","        texto_a_analizar = translator.translate(texto, dest='en').text\n","        texto_esp = translator.translate(texto, dest='es').text\n","        \n","    texto_preprocesado = ' '.join(preprocessing_function(texto_a_analizar))\n","    texto_vectorizado = vect.transform([texto_preprocesado])\n","\n","    prediccion = model.predict(texto_vectorizado)\n","\n","    print('{0: <30}'.format('Texto original') + ': ' + texto)\n","    if(language not in 'es'):\n","        print('{0: <30}'.format('Texto traducido') + ': ' + texto_esp)\n","    print('{0: <30}'.format('Texto utilizado para predict') + ': ' + texto_a_analizar)\n","    print('------------------------------------------------------------------------------------------------------')\n"]},{"cell_type":"markdown","metadata":{},"source":["# Palabras mas comunes segun mi funcion analizadora"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Usemos Lemmatization:\n","from collections import Counter\n","\n","wnl = WordNetLemmatizer()\n","\n","word_counts = Counter()\n","\n","for row in tqdm(dataframe.iterrows(), total=len(dataframe)):\n","    text = row[1].iat[0]\n","    for word in preprocessing_function(text):\n","        word_counts[word] += 1\n","\n","len(word_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_counts.most_common(1000)"]},{"cell_type":"markdown","metadata":{},"source":["# Verificar ocurrencia de palabras"]},{"cell_type":"markdown","metadata":{},"source":["## Palabras mas comunes contando palabra por palabra"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Intuición principal en este tipo de tecnicas: Contar las ocurrencias de las palabras.\n","from collections import defaultdict, Counter\n","\n","# Instanciamos un contador de python\n","word_counts = Counter()\n","\n","for row in tqdm(dataframe.iterrows(), total=len(dataframe)):\n","    text = row[1]\n","\n","    # Usamos la funcion implementada en pandas split() para separar palabras por espacios en blanco.\n","    for word in text.str.split()[0]:\n","        word_counts[word] += 1\n","\n","len(word_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_counts.most_common(25)"]},{"cell_type":"markdown","metadata":{},"source":["## Palabras mas comunes contando con tokenizador"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from nltk import word_tokenize\n","\n","word_counts = Counter()\n","\n","for row in tqdm(dataframe.iterrows(), total=len(dataframe)):\n","    text = row[1].iat[0]\n","    for word in word_tokenize(text):\n","        word_counts[word] += 1\n","\n","len(word_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_counts.most_common(25)"]},{"cell_type":"markdown","metadata":{},"source":["## Palabras mas comunes contando teniendo en cuenta stopwords"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["stopwords_en = stopwords.words('english')\n","print(stopwords_en)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from nltk.corpus import stopwords\n","\n","stopwords_en = stopwords.words('english')\n","\n","word_counts = Counter()\n","\n","for row in tqdm(dataframe.iterrows(), total=len(dataframe)):\n","    text = row[1].iat[0]\n","\n","    for word in word_tokenize(text):\n","        word_lowercase = word.lower()\n","\n","        if word_lowercase not in stopwords_en:\n","            word_counts[word_lowercase] += 1\n","\n","len(word_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_counts.most_common(10)"]},{"cell_type":"markdown","metadata":{},"source":["## Palabras mas comunes contando teniendo en cuenta signos de puntuacion"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from string import punctuation\n","\n","# Hacemos una union entre conjunto de caracteres de puntuacion nativos a nuestro conjunto de stopwords usando la operation union de sets de datos.\n","stopwords_en = set(stopwords_en).union(set(punctuation))\n","\n","word_counts = Counter()\n","\n","for row in tqdm(dataframe.iterrows(), total=len(dataframe)):\n","    text = row[1].iat[0]\n","\n","    for word in word_tokenize(text):\n","        word_lowercase = word.lower()\n","\n","        if word_lowercase not in stopwords_en:\n","            word_counts[word_lowercase] += 1\n","\n","len(word_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_counts.most_common(10)"]},{"cell_type":"markdown","metadata":{},"source":["## Stemming"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Usamos Stemming\n","from nltk.stem import PorterStemmer\n","\n","porter = PorterStemmer()\n","\n","word_counts = Counter()\n","\n","for row in tqdm(dataframe.iterrows(), total=len(dataframe)):\n","    text = row[1].iat[0]\n","\n","    for word in word_tokenize(text):\n","        word_lowercase = word.lower()\n","\n","        if word_lowercase not in stopwords_en:\n","            stemmed_word = porter.stem(word_lowercase)\n","            word_counts[stemmed_word] += 1\n","\n","len(word_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_counts.most_common(10)"]},{"cell_type":"markdown","metadata":{},"source":["## Lematization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Usemos Lemmatization:\n","from nltk import word_tokenize\n","from nltk import pos_tag\n","from nltk.stem import WordNetLemmatizer\n","\n","\n","wnl = WordNetLemmatizer()\n","\n","def penn2morphy(penntag):\n","    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n","    morphy_tag = {'NN':'n', 'JJ':'a',\n","                  'VB':'v', 'RB':'r'}\n","    try:\n","        return morphy_tag[penntag[:2]]\n","    except:\n","        return 'n'\n","\n","word_counts = Counter()\n","\n","for row in tqdm(dataframe.iterrows(), total=len(dataframe)):\n","    text = row[1].iat[0]\n","\n","    for word, tag in pos_tag(word_tokenize(text)):\n","        word_lemmatized = wnl.lemmatize(word.lower(), pos=penn2morphy(tag))\n","\n","        if word_lemmatized not in stopwords_en:\n","            word_counts[word_lemmatized] += 1\n","\n","len(word_counts)"]}],"metadata":{"colab":{"provenance":[{"file_id":"1Uo1bHOz9RJfRzCG9S_8o3b1egLTtfyIW","timestamp":1714694532835},{"file_id":"14rAtWC1QzcGMRBgdRjrogYipZ2_RwVbQ","timestamp":1714693907215},{"file_id":"1bdtNIkY_2x9V_MbWLOtI0Qc-Jqm0bBwe","timestamp":1714222939750},{"file_id":"1CZK1XIyCXl3c51Uo3uy50EQqfnAFY1e6","timestamp":1714079362679}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}
