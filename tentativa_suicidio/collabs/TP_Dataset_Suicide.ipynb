{"cells":[{"cell_type":"markdown","metadata":{"id":"xwDzlfPptSwb"},"source":["# Librerias"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4197,"status":"ok","timestamp":1714742773677,"user":{"displayName":"Victor Povoli Olivera","userId":"04383995252204734605"},"user_tz":180},"id":"8Q8yzQs0oM5z","outputId":"5d59bed7-d10c-476d-cc89-b3e4754a5237"},"outputs":[],"source":["# trabajar con datos tabulares\n","import pandas as pd\n","# nlp\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","nltk.download('word2vec_sample')\n","# guardado del modelo entranado\n","import joblib\n","# eliminar warning del replace\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","# traducir\n","from googletrans import Translator\n","# emojis\n","import emoji\n","# enum\n","from enum import Enum\n","\n","def is_running_on_colab():\n","    try:\n","        import google.colab\n","        return True\n","    except ImportError:\n","        return False"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n","\n","class Modelos(Enum):\n","    LOGISTIC_REGRESSION = 'logistic_regression'\n","    DECISION_TREE = 'decision_tree'\n","    MULTINOMIAL = 'multinomial'\n","    BERNOULLI = 'bernoulli'\n","    GAUSIAN = 'gausian'\n","\n","def select_model_to_train(model_name):\n","    models = {\n","        Modelos.LOGISTIC_REGRESSION.value: LogisticRegression(max_iter=1000),\n","        Modelos.DECISION_TREE.value : DecisionTreeClassifier(),\n","        Modelos.MULTINOMIAL.value : MultinomialNB(),\n","        Modelos.BERNOULLI.value: BernoulliNB(),\n","        Modelos.GAUSIAN.value: GaussianNB()\n","    }\n","    if model_name in models:\n","        return models[model_name]\n","    else:\n","        raise ValueError(f\"Modelo '{model_name}' no válido\")"]},{"cell_type":"markdown","metadata":{},"source":["# Funcion procesador texto"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from nltk import pos_tag, word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","from string import punctuation\n","\n","wnl = WordNetLemmatizer()\n","\n","def penn2morphy(penntag):\n","    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n","    morphy_tag = {'NN':'n', 'JJ':'a',\n","                  'VB':'v', 'RB':'r'}\n","    try:\n","        return morphy_tag[penntag[:2]]\n","    except:\n","        return 'n'\n","\n","\n","stopwords_en = stopwords.words('english')\n","stopwords_en = set(stopwords_en).union(set(punctuation))\n","\n","my_custom_stopwords = {'’', \"n't\", \"'m\", \"'s\", \"'ve\", '...', 'ca', \"''\", '``', '\\u200d', 'im', 'na', \"'ll\", '..', 'u', \"'re\", \"'d\", '--', '”', '“', '\\u200f\\u200f\\u200e', '....', 'ㅤ','\\u200e\\u200f\\u200f\\u200e', 'x200b', 'ive', '.-', '\\u200e', '‘'}\n","\n","stopwords_en = stopwords_en.union(my_custom_stopwords)\n","\n","\n","def preprocessing_function(text):\n","    words = []\n","\n","    for word, tag in pos_tag(word_tokenize(text)):\n","        word_lemmatized = wnl.lemmatize(word.lower(), pos=penn2morphy(tag))\n","\n","        if '\\u200b' in word_lemmatized:\n","            continue\n","\n","        if word_lemmatized not in stopwords_en and not word_lemmatized.isdigit() and not emoji.purely_emoji(word_lemmatized):\n","            words.append(word_lemmatized)\n","\n","    return words"]},{"cell_type":"markdown","metadata":{},"source":["# Funcion test"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","import matplotlib.pyplot as plt\n","\n","def test(clf, nombreModelo):\n","  clf.fit(X_train_vectorized.toarray(), y_train)\n","  y_pred = clf.predict(X_test_vectorized.toarray())\n","\n","  accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n","\n","  disp = ConfusionMatrixDisplay.from_estimator(\n","        clf, X_test_vectorized.toarray(), y_test,  xticks_rotation=\"vertical\"\n","   )\n","  \n","  plt.title(nombreModelo + \" - Accuracy: {:.2f}\".format(accuracy))\n","\n","  # disp.plot()\n","  plt.show()\n","\n","  return disp"]},{"cell_type":"markdown","metadata":{"id":"SxPDjNHatZYY"},"source":["# Importacion dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":280},"executionInfo":{"elapsed":278,"status":"error","timestamp":1714742937245,"user":{"displayName":"Victor Povoli Olivera","userId":"04383995252204734605"},"user_tz":180},"id":"c62w5_G2oVUg","outputId":"1d0967bc-7010-40aa-ab60-0f2c9449f060"},"outputs":[],"source":["cant_importada = 50000\n","path_base_dataset = '/content/' if is_running_on_colab() else '..\\\\datasets\\\\'\n","path_dataset =  path_base_dataset + 'Suicide_Detection.csv'\n","\n","dataframe = pd.read_csv(path_dataset, nrows=cant_importada)\n","\n","print(dataframe)"]},{"cell_type":"markdown","metadata":{},"source":["O lo descargo de internet"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from datasets import load_dataset\n","from sklearn.model_selection import train_test_split\n","\n","# jquiros/suicide el que usamos todo el tiempo\n","dataset = load_dataset(\"vibhorag101/suicide_prediction_dataset_phr\", \"default\")\n","\n","dataset = None\n","\n","print(dataframe)"]},{"cell_type":"markdown","metadata":{},"source":["# Checkeo y formateo del dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataframe['class'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Este caso puntual el csv la primera columna es el indice que no nos interesa, si quiero eliminarla por el nombre que le asigna pandas\n","try:\n","    dataframe = dataframe.drop('Unnamed: 0', axis=1)\n","except:\n","    print(\"No se elimino columna Unnamed: 0\")\n","\n","# o eliminarla por la posicion\n","# dataframe = dataframe.drop(dataframe.columns[0], axis=1)\n","\n","# Paso a booleano la clasificacion\n","dataframe['class'] = dataframe['class'].replace({\"suicide\": True, \"non-suicide\": False})\n","\n","dataframe.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Obtener los conteos de las clases y sus respectivos índices\n","class_counts = dataframe['class'].value_counts()\n","class_indices = class_counts.index.values\n","\n","# Crear el gráfico de barras\n","plt.bar(class_indices, class_counts, color=['blue', 'green'])\n","\n","# Añadir etiquetas y título\n","plt.xlabel('suicide - non suicide')\n","plt.ylabel('Cantidad')\n","plt.title(\"IA APLICADA\")\n","\n","# Mostrar el gráfico\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"pyOZvVNZ0Cne"},"source":["# Entrenamiento y guardado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"91LdWl8I0Eth"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","\n","X_train, X_test, y_train, y_test = train_test_split(dataframe[\"text\"],\n","                                                    dataframe[\"class\"],\n","                                                    test_size=0.20, random_state=0,\n","                                                    stratify=dataframe[\"class\"])\n","\n","len(y_train), len(y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cBijtjrz0M8A"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","\n","# ----------------------------------------------------------------------\n","#BoW con vectores binarios. Estos se usaban en tareas de analisis de sentimientos que no necesita saber la cantidad de veces que se repite una palabra sino su mera presencia.\n","count_vectorizer = CountVectorizer(analyzer=preprocessing_function, binary=True)\n","\n","# La idea es dado un texto hay que considerarlo una colección o bolsa (Bag) de palabras ignorando el orden y contexto.\n","# count_vectorizer = CountVectorizer(analyzer=preprocessing_function)\n","\n","#Term Frecuency - Inverse Document Frecuency trata este tema calculando la importancia de una palabra en base a las otras en el documento y en el corpus.\n","# count_vectorizer = TfidfVectorizer(analyzer=preprocessing_function)\n","\n","# ----------------------------------------------------------------------\n","\n","X_train_vectorized = count_vectorizer.fit_transform(X_train)\n","X_test_vectorized = count_vectorizer.transform(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["modelos_disponibles = {\n","    Modelos.LOGISTIC_REGRESSION.value : True,\n","    Modelos.DECISION_TREE.value: True,\n","    Modelos.MULTINOMIAL.value: True,\n","    Modelos.BERNOULLI.value: True,\n","    Modelos.GAUSIAN.value: True,\n","}\n","\n","cant_entrenada_str = \"{:.0f}\".format(cant_importada/1000) + 'k'\n","\n","for nombreModelo, entrenar in modelos_disponibles.items():\n","    if entrenar:\n","        modelToTrain = select_model_to_train(nombreModelo)\n","        test(modelToTrain, nombreModelo)\n","\n","        path_base_modelo = '/content/' if is_running_on_colab() else '..\\\\entrenados\\\\'\n","        path_modelo = path_base_modelo + nombreModelo + '_' + cant_entrenada_str\n","        joblib.dump(modelToTrain, path_modelo +  '_model.pkl')\n","        joblib.dump(count_vectorizer, path_modelo + '_vector.pkl')\n","        print(f\"Modelo {nombreModelo} guardado en \" + path_modelo)\n","        modelToTrain = None\n","    else:\n","        print(f\"Modelo {nombreModelo} no entrenado\")\n","    print(\"--------------------------------\")"]},{"cell_type":"markdown","metadata":{"id":"-4Ubam3DGzhI"},"source":["# Casos de uso"]},{"cell_type":"markdown","metadata":{},"source":["## Cargar modelo previamente generado"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nombre_modelo_prev_entrenado = Modelos.LOGISTIC_REGRESSION.value\n","# usar formato '25k' para 25.000 filas ejemplo\n","cant_prev_entrenada = '10k'\n","\n","path_base_modelo_generado = '/content/' if is_running_on_colab() else '..\\\\entrenados\\\\'\n","path_modelo_generado = path_base_modelo_generado + nombre_modelo_prev_entrenado + '_' + cant_prev_entrenada\n","\n","loaded_model = joblib.load(path_modelo_generado + '_model.pkl')\n","loaded_count_vectorizer = joblib.load(path_modelo_generado + '_vector.pkl')\n","\n","vect = loaded_count_vectorizer\n","model = loaded_model\n","\n","print(type(vect))\n","print(type(model))\n","print(model.n_features_in_)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Probar con dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","path_base_dataset = '/content/' if is_running_on_colab() else '..\\\\datasets\\\\'\n","path_dataset =  path_base_dataset + 'Suicide_Detection.csv'\n","\n","cant_a_probar = 5000\n","\n","dataframeTest = pd.read_csv(path_dataset, skiprows=40000, nrows=cant_a_probar)\n","dataframeTest.columns = [\"borrar\",\"text\",\"class\"]\n","dataframeTest = dataframeTest.drop(\"borrar\", axis=1)\n","dataframeTest['class'] = dataframeTest['class'].replace({\"suicide\": True, \"non-suicide\": False})\n","\n","print(dataframeTest)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["counter = 0\n","\n","for index, row in dataframeTest.iterrows():\n","    texto = row['text']\n","    \n","    # translator = Translator()\n","    # traduccion = translator.translate(texto, dest='es').text\n","\n","    texto_preprocesado = preprocessing_function(texto)\n","    texto_preprocesado_str = ' '.join(texto_preprocesado)\n","\n","    texto_vectorizado = vect.transform([texto_preprocesado_str])\n","\n","    prediccion = model.predict(texto_vectorizado)\n","\n","    clase_real = row['class']\n","    resultado_prediccion = 'suicida' if prediccion else 'no suicida'\n","    resultado_real = 'suicida' if clase_real else 'no suicida'\n","\n","    if resultado_real != resultado_prediccion :\n","        counter = counter + 1\n","    #     print(\"----------------\")\n","    #     print(texto)\n","    #     print(\"Dato dataset \" + resultado_real)\n","    #     print(\"Dato prediccion \" + resultado_prediccion)\n","\n","print(\"La cantidad de casos donde no coincidio la prediccion con la clasificacion real del dataset: \" + str(counter) + \" - \" + str(\"{:.2f}\".format(cant_a_probar / counter) if counter != 0 else \"no hubo errores\") + \"%\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Probar con nuestro datos"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["textos_prueba = [\n","                \"I want to jump from a bridge\",\n","                \"I want to suicide me\",\n","                 \"I hate my parents with all my heart\",\n","                 \"I hate all about this life\",\n","                 \"I cry every night\",\n","                 \"I don't know what is happen to me, but I don't want live anymore\"\n","                 ]\n","\n","translator = Translator()\n","\n","for texto in textos_prueba:\n","    traduccion = translator.translate(texto, dest='es').text\n","\n","    texto_preprocesado = preprocessing_function(texto)\n","    texto_preprocesado_str = ' '.join(texto_preprocesado)\n","    print(texto_preprocesado_str)\n","\n","    # decision tree\n","    texto_vectorizado = vect.transform([texto_preprocesado_str])\n","\n","    prediccion = model.predict(texto_vectorizado)\n","\n","    print(f\"'{texto}'\\n'{traduccion}'\\n{'suicida' if prediccion else 'no suicida'}\\n\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["# Verificar ocurrencia de palabras"]},{"cell_type":"markdown","metadata":{},"source":["## Palabras mas comunes contando palabra por palabra"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Intuición principal en este tipo de tecnicas: Contar las ocurrencias de las palabras.\n","from collections import defaultdict, Counter\n","from tqdm import tqdm\n","\n","# Instanciamos un contador de python\n","word_counts = Counter()\n","\n","for row in tqdm(dataframe.iterrows(), total=len(dataframe)):\n","    text = row[1]\n","\n","    # Usamos la funcion implementada en pandas split() para separar palabras por espacios en blanco.\n","    for word in text.str.split()[0]:\n","        word_counts[word] += 1\n","\n","len(word_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_counts.most_common(25)"]},{"cell_type":"markdown","metadata":{},"source":["## Palabras mas comunes contando con tokenizador"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from nltk import word_tokenize\n","\n","word_counts = Counter()\n","\n","for row in tqdm(dataframe.iterrows(), total=len(dataframe)):\n","    text = row[1].iat[0]\n","    for word in word_tokenize(text):\n","        word_counts[word] += 1\n","\n","len(word_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_counts.most_common(25)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print((\"hello, how are you?\").split())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(word_tokenize(\"hello, how are you?\"))"]},{"cell_type":"markdown","metadata":{},"source":["## Palabras mas comunes contando teniendo en cuenta stopwords"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["stopwords_en = stopwords.words('english')\n","print(stopwords_en)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from nltk.corpus import stopwords\n","\n","stopwords_en = stopwords.words('english')\n","\n","word_counts = Counter()\n","\n","for row in tqdm(dataframe.iterrows(), total=len(dataframe)):\n","    text = row[1].iat[0]\n","\n","    for word in word_tokenize(text):\n","        word_lowercase = word.lower()\n","\n","        if word_lowercase not in stopwords_en:\n","            word_counts[word_lowercase] += 1\n","\n","len(word_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_counts.most_common(10)"]},{"cell_type":"markdown","metadata":{},"source":["## Palabras mas comunes contando teniendo en cuenta signos de puntuacion"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from string import punctuation\n","\n","# Hacemos una union entre conjunto de caracteres de puntuacion nativos a nuestro conjunto de stopwords usando la operation union de sets de datos.\n","stopwords_en = set(stopwords_en).union(set(punctuation))\n","\n","word_counts = Counter()\n","\n","for row in tqdm(dataframe.iterrows(), total=len(dataframe)):\n","    text = row[1].iat[0]\n","\n","    for word in word_tokenize(text):\n","        word_lowercase = word.lower()\n","\n","        if word_lowercase not in stopwords_en:\n","            word_counts[word_lowercase] += 1\n","\n","len(word_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_counts.most_common(10)"]},{"cell_type":"markdown","metadata":{},"source":["## Stemming"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Usamos Stemming\n","from nltk.stem import PorterStemmer\n","\n","porter = PorterStemmer()\n","\n","word_counts = Counter()\n","\n","for row in tqdm(dataframe.iterrows(), total=len(dataframe)):\n","    text = row[1].iat[0]\n","\n","    for word in word_tokenize(text):\n","        word_lowercase = word.lower()\n","\n","        if word_lowercase not in stopwords_en:\n","            stemmed_word = porter.stem(word_lowercase)\n","            word_counts[stemmed_word] += 1\n","\n","len(word_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_counts.most_common(10)"]},{"cell_type":"markdown","metadata":{},"source":["## Lematization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Usemos Lemmatization:\n","from nltk import word_tokenize\n","from nltk import pos_tag\n","from nltk.stem import WordNetLemmatizer\n","\n","\n","wnl = WordNetLemmatizer()\n","\n","def penn2morphy(penntag):\n","    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n","    morphy_tag = {'NN':'n', 'JJ':'a',\n","                  'VB':'v', 'RB':'r'}\n","    try:\n","        return morphy_tag[penntag[:2]]\n","    except:\n","        return 'n'\n","\n","word_counts = Counter()\n","\n","for row in tqdm(dataframe.iterrows(), total=len(dataframe)):\n","    text = row[1].iat[0]\n","\n","    for word, tag in pos_tag(word_tokenize(text)):\n","        word_lemmatized = wnl.lemmatize(word.lower(), pos=penn2morphy(tag))\n","\n","        if word_lemmatized not in stopwords_en:\n","            word_counts[word_lemmatized] += 1\n","\n","len(word_counts)"]},{"cell_type":"markdown","metadata":{},"source":["## Palabras mas comunes segun mi funcion analizadora"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Usemos Lemmatization:\n","from collections import Counter\n","from tqdm import tqdm\n","from nltk import word_tokenize\n","from nltk import pos_tag\n","from nltk.stem import WordNetLemmatizer\n","\n","word_counts = Counter()\n","\n","for row in tqdm(dataframe.iterrows(), total=len(dataframe)):\n","    text = row[1].iat[0]\n","\n","    for word, tag in pos_tag(word_tokenize(text)):\n","        word_lemmatized = wnl.lemmatize(word.lower(), pos=penn2morphy(tag))\n","\n","        if '\\u200b' in word_lemmatized:\n","            continue\n","\n","        if word_lemmatized not in stopwords_en and not word_lemmatized.isdigit() and not emoji.purely_emoji(word_lemmatized):\n","            word_counts[word_lemmatized] += 1\n","\n","len(word_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_counts.most_common(1000)"]},{"cell_type":"markdown","metadata":{},"source":["# Entrenamientos por clasificador"]},{"cell_type":"markdown","metadata":{},"source":["### Decision Tree"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","dtc = DecisionTreeClassifier()\n","test(dtc)"]},{"cell_type":"markdown","metadata":{},"source":["### MultinomialBN"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.naive_bayes import MultinomialNB\n","mnb = MultinomialNB()\n","test(mnb)"]},{"cell_type":"markdown","metadata":{},"source":["### BernoulliNB"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.naive_bayes import BernoulliNB\n","bnb = BernoulliNB()\n","test(bnb)"]},{"cell_type":"markdown","metadata":{},"source":["### Logistic Regression"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","# lgr = LogisticRegression()\n","lgr = LogisticRegression(max_iter=1000)\n","test(lgr)"]},{"cell_type":"markdown","metadata":{},"source":["### Gaussian"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.naive_bayes import GaussianNB\n","gnb = GaussianNB()\n","test(gnb)"]},{"cell_type":"markdown","metadata":{},"source":["### Random Forest Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","clf = RandomForestClassifier(max_depth=2, random_state=42)\n","test(clf)"]}],"metadata":{"colab":{"provenance":[{"file_id":"1Uo1bHOz9RJfRzCG9S_8o3b1egLTtfyIW","timestamp":1714694532835},{"file_id":"14rAtWC1QzcGMRBgdRjrogYipZ2_RwVbQ","timestamp":1714693907215},{"file_id":"1bdtNIkY_2x9V_MbWLOtI0Qc-Jqm0bBwe","timestamp":1714222939750},{"file_id":"1CZK1XIyCXl3c51Uo3uy50EQqfnAFY1e6","timestamp":1714079362679}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"}},"nbformat":4,"nbformat_minor":0}
