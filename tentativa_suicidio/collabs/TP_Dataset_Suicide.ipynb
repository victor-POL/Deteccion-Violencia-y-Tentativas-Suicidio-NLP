{"cells":[{"cell_type":"markdown","metadata":{"id":"xwDzlfPptSwb"},"source":["# Librerias"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4197,"status":"ok","timestamp":1714742773677,"user":{"displayName":"Victor Povoli Olivera","userId":"04383995252204734605"},"user_tz":180},"id":"8Q8yzQs0oM5z","outputId":"5d59bed7-d10c-476d-cc89-b3e4754a5237"},"outputs":[],"source":["# trabajar con datos tabulares\n","import pandas as pd\n","# nlp\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","nltk.download('word2vec_sample')\n","# guardado del modelo entranado\n","import pickle\n","import joblib\n","# eliminar warning del replace\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","# traducir\n","from googletrans import Translator\n","# emojis\n","import emoji"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def is_running_on_colab():\n","    try:\n","        import google.colab\n","        return True\n","    except ImportError:\n","        return False"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n","\n","def select_model_to_train(model_name):\n","    models = {\n","        'logistic_regression': LogisticRegression(max_iter=1000),\n","        'decision_tree': DecisionTreeClassifier(),\n","        'multinomial': MultinomialNB(),\n","        'bernoulli': BernoulliNB(),\n","        'gausian': GaussianNB()\n","    }\n","    if model_name in models:\n","        return models[model_name]\n","    else:\n","        raise ValueError(f\"Modelo '{model_name}' no válido\")"]},{"cell_type":"markdown","metadata":{},"source":["# Definir que entrenar"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# modelo, entrenar?\n","modelos_disponibles = {\n","    'logistic_regression': True,\n","    'decision_tree': True,\n","    'multinomial': True,\n","    'bernoulli': True,\n","    'gausian': True,\n","}"]},{"cell_type":"markdown","metadata":{},"source":["# Funcion analizadora"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from nltk import pos_tag, word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","from string import punctuation\n","\n","# lematization\n","wnl = WordNetLemmatizer()\n","\n","def penn2morphy(penntag):\n","    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n","    morphy_tag = {'NN':'n', 'JJ':'a',\n","                  'VB':'v', 'RB':'r'}\n","    try:\n","        return morphy_tag[penntag[:2]]\n","    except:\n","        return 'n'\n","\n","\n","# Agrego a stopwords signos de puntuacion y emojis\n","stopwords_en = stopwords.words('english')\n","stopwords_en = set(stopwords_en).union(set(punctuation))\n","# stopwords_en = set(stopwords_en).union(set(emoji.UNICODE_EMOJI['en']))\n","\n","# Defino la funcion\n","def preprocessing_function(text):\n","    words = []\n","\n","    for word, tag in pos_tag(word_tokenize(text)):\n","        word_lemmatized = wnl.lemmatize(word.lower(), pos=penn2morphy(tag))\n","\n","        if word_lemmatized not in stopwords_en and not word_lemmatized.isdigit() and not emoji.purely_emoji(word_lemmatized):\n","            words.append(word_lemmatized)\n","\n","    return words"]},{"cell_type":"markdown","metadata":{},"source":["# Funcion test"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Vamos a definir nuestra funcion de test y graficar nuestra confusion matrix.\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","import matplotlib.pyplot as plt\n","\n","def test(clf, nombreModelo):\n","  clf.fit(X_train_vectorized.toarray(), y_train)\n","  y_pred = clf.predict(X_test_vectorized.toarray())\n","\n","  accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n","  # Crear matriz de confusión y mostrarla\n","  disp = ConfusionMatrixDisplay.from_estimator(\n","        clf, X_test_vectorized.toarray(), y_test,  xticks_rotation=\"vertical\"\n","   )\n","  \n","  # Añadir título a la matriz de confusión\n","  plt.title(nombreModelo + \" - Accuracy: {:.2f}\".format(accuracy))\n","\n","  # Mostrar la matriz de confusión con el título\n","  # disp.plot()\n","  plt.show()\n","\n","  return disp"]},{"cell_type":"markdown","metadata":{"id":"SxPDjNHatZYY"},"source":["# Importacion dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":280},"executionInfo":{"elapsed":278,"status":"error","timestamp":1714742937245,"user":{"displayName":"Victor Povoli Olivera","userId":"04383995252204734605"},"user_tz":180},"id":"c62w5_G2oVUg","outputId":"1d0967bc-7010-40aa-ab60-0f2c9449f060"},"outputs":[],"source":["# Ruta del archivo CSV\n","cant_importada = 1000\n","file_path_base = './content/' if is_running_on_colab() else '..\\\\datasets\\\\'\n","file_path =  file_path_base + 'Suicide_Detection.csv'\n","\n","# Leer el archivo CSV en un DataFrame de Pandas, \n","# dataframe = pd.read_csv(file_path)\n","# si quiero limitar la cantidad a importar\n","dataframe = pd.read_csv(file_path, nrows=cant_importada)\n","\n","# Este caso puntual el csv la primera columna es el indice que no nos interesa, si quiero eliminarla por el nombre que le asigna pandas\n","# dataframe = dataframe.drop('Unnamed: 0', axis=1)\n","# o eliminarla por la posicion\n","dataframe = dataframe.drop(dataframe.columns[0], axis=1)\n","\n","# Paso a booleano la clasificacion\n","dataframe['class'] = dataframe['class'].replace({\"suicide\": True, \"non-suicide\": False})\n","\n","print(dataframe)"]},{"cell_type":"markdown","metadata":{"id":"pyOZvVNZ0Cne"},"source":["# Entrenamiento con vectorizador"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"91LdWl8I0Eth"},"outputs":[],"source":["# Hacemos un split de sets de train y test\n","from sklearn.model_selection import train_test_split\n","\n","\n","X_train, X_test, y_train, y_test = train_test_split(dataframe[\"text\"],\n","                                                    dataframe[\"class\"],\n","                                                    test_size=0.15, random_state=0,\n","                                                    stratify=dataframe[\"class\"])\n","\n","len(y_train), len(y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cBijtjrz0M8A"},"outputs":[],"source":["# Sklearn tiene un objeto llamado CountVectorizer que nos permite pasarle un \"analyzer\"\n","# El \"analyzer\" toma el texto que le pasamos y devuelve una lista de palabras a contar.\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","count_vectorizer = CountVectorizer(analyzer=preprocessing_function)\n","\n","# Entrenamos nuestro CountVectorizer en el training set and transformamos ambos datasets\n","X_train_vectorized = count_vectorizer.fit_transform(X_train)\n","X_test_vectorized = count_vectorizer.transform(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["# Entrenamiento y Guardado"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cant_entrenada_str = \"{:.0f}\".format(cant_importada/1000) + 'k'\n","\n","for nombreModelo, entrenar in modelos_disponibles.items():\n","    if entrenar:\n","        modelToTrain = select_model_to_train(nombreModelo)\n","        test(modelToTrain, nombreModelo)\n","\n","        path_base = './content/' if is_running_on_colab() else '..\\\\entrenados\\\\'\n","        path = path_base + nombreModelo + '_' + cant_entrenada_str\n","        joblib.dump(modelToTrain, path +  '_model.pkl')\n","        joblib.dump(count_vectorizer, path + '_vector.pkl')\n","        print(f\"Modelo {nombreModelo} guardado en \" + path)\n","        modelToTrain = None\n","    else:\n","        print(f\"Modelo {nombreModelo} no entrenado\")\n","    print(\"--------------------------------\")"]},{"cell_type":"markdown","metadata":{"id":"-4Ubam3DGzhI"},"source":["# Casos de uso"]},{"cell_type":"markdown","metadata":{},"source":["## Cargar modelo recien generado"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vect = count_vectorizer\n","model = lgr"]},{"cell_type":"markdown","metadata":{},"source":["## Cargar modelo previamente generado"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# verificar si ya existe una carpeta en dump o joblib con el nombre del modelo que se esta guardando\n","# decision_tree         -> dtc\n","# multinomial           -> mnb\n","# bernoulli             -> bnb\n","# gausian               -> gnb\n","# logistic_regression   -> lgr\n","nombre_modelo_prev_entrenado = 'decision_tree'\n","# usar formato '25k' para 25.000 filas ejemplo\n","cant_prev_entrenada = '50k'\n","\n","path = '..\\\\entrenados\\\\' + nombre_modelo_prev_entrenado + '_' + cant_prev_entrenada\n","\n","loaded_model = joblib.load(path + '_model.pkl')\n","loaded_count_vectorizer = joblib.load(path + '_vector.pkl')\n","\n","vect = loaded_count_vectorizer\n","model = loaded_model\n","\n","print(type(model))\n","print(model.n_features_in_)\n","print(model.max_features_)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Probar con dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["file_path = '..\\\\datasets\\\\Suicide_Detection.csv'\n","cant_a_probar = 5000\n","\n","dataframeTest = pd.read_csv(file_path, skiprows=50000, nrows=cant_a_probar)\n","dataframeTest.columns = [\"borrar\",\"text\",\"class\"]\n","dataframeTest = dataframeTest.drop(\"borrar\", axis=1)\n","dataframeTest['class'] = dataframeTest['class'].replace({\"suicide\": True, \"non-suicide\": False})\n","\n","print(dataframeTest)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["counter = 0\n","\n","for index, row in dataframeTest.iterrows():\n","    texto = row['text']\n","    \n","    # translator = Translator()\n","    # traduccion = translator.translate(texto, dest='es').text\n","\n","    texto_preprocesado = preprocessing_function(texto)\n","    texto_preprocesado_str = ' '.join(texto_preprocesado)\n","\n","    texto_vectorizado = vect.transform([texto_preprocesado_str])\n","\n","    prediccion = model.predict(texto_vectorizado)\n","\n","    clase_real = row['class']\n","    resultado_prediccion = 'suicida' if prediccion else 'no suicida'\n","    resultado_real = 'suicida' if clase_real else 'no suicida'\n","\n","    if resultado_real != resultado_prediccion :\n","        counter = counter + 1\n","    #     print(\"----------------\")\n","    #     print(texto)\n","    #     print(\"Dato dataset \" + resultado_real)\n","    #     print(\"Dato prediccion \" + resultado_prediccion)\n","\n","print(\"La cantidad de casos donde no coincidio la prediccion con la clasificacion real del dataset: \" + str(counter) + \" \" + str(cant_a_probar / counter if counter != 0 else \"no hubo errores\") + \"%\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Probar con nuestro datos"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["textos_prueba = [\n","                \"I want to jump from a bridge\",\n","                \"I want to suicide me\",\n","                 \"I hate my parents with all my heart\",\n","                 \"I hate all about this life\",\n","                 \"I cry every night\",\n","                 \"I don't know what is happen to me, but I don't want live anymore\"\n","                 ]\n","\n","translator = Translator()\n","\n","for texto in textos_prueba:\n","    traduccion = translator.translate(texto, dest='es').text\n","    print(traduccion)\n","\n","    texto_preprocesado = preprocessing_function(texto)\n","    texto_preprocesado_str = ' '.join(texto_preprocesado)\n","\n","    # decision tree\n","    texto_vectorizado = vect.transform([texto_preprocesado_str])\n","\n","    prediccion = model.predict(texto_vectorizado)\n","\n","    print(f\"Predicción para el texto '{texto}': {'suicida' if prediccion else 'no suicida'}\\n\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["# Verificar ocurrencia de palabras"]},{"cell_type":"markdown","metadata":{},"source":["## Palabras mas comunes contando palabra por palabra"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Intuición principal en este tipo de tecnicas: Contar las ocurrencias de las palabras.\n","from collections import defaultdict, Counter\n","from tqdm import tqdm\n","\n","# Instanciamos un contador de python\n","word_counts = Counter()\n","\n","for row in tqdm(dataframe.iterrows(), total=len(dataframe)):\n","    text = row[1]\n","\n","    # Usamos la funcion implementada en pandas split() para separar palabras por espacios en blanco.\n","    for word in text.str.split()[0]:\n","        word_counts[word] += 1\n","\n","len(word_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_counts.most_common(25)"]},{"cell_type":"markdown","metadata":{},"source":["## Palabras mas comunes contando con tokenizador"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from nltk import word_tokenize\n","\n","word_counts = Counter()\n","\n","for row in tqdm(dataframe.iterrows(), total=len(dataframe)):\n","    text = row[1].iat[0]\n","    for word in word_tokenize(text):\n","        word_counts[word] += 1\n","\n","len(word_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_counts.most_common(25)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print((\"hello, how are you?\").split())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(word_tokenize(\"hello, how are you?\"))"]},{"cell_type":"markdown","metadata":{},"source":["## Palabras mas comunes contando teniendo en cuenta stopwords"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["stopwords_en = stopwords.words('english')\n","print(stopwords_en)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from nltk.corpus import stopwords\n","\n","stopwords_en = stopwords.words('english')\n","\n","word_counts = Counter()\n","\n","for row in tqdm(dataframe.iterrows(), total=len(dataframe)):\n","    text = row[1].iat[0]\n","\n","    for word in word_tokenize(text):\n","        word_lowercase = word.lower()\n","\n","        if word_lowercase not in stopwords_en:\n","            word_counts[word_lowercase] += 1\n","\n","len(word_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_counts.most_common(10)"]},{"cell_type":"markdown","metadata":{},"source":["## Palabras mas comunes contando teniendo en cuenta signos de puntuacion"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from string import punctuation\n","\n","# Hacemos una union entre conjunto de caracteres de puntuacion nativos a nuestro conjunto de stopwords usando la operation union de sets de datos.\n","stopwords_en = set(stopwords_en).union(set(punctuation))\n","\n","word_counts = Counter()\n","\n","for row in tqdm(dataframe.iterrows(), total=len(dataframe)):\n","    text = row[1].iat[0]\n","\n","    for word in word_tokenize(text):\n","        word_lowercase = word.lower()\n","\n","        if word_lowercase not in stopwords_en:\n","            word_counts[word_lowercase] += 1\n","\n","len(word_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_counts.most_common(10)"]},{"cell_type":"markdown","metadata":{},"source":["## Stemming"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Usamos Stemming\n","from nltk.stem import PorterStemmer\n","\n","porter = PorterStemmer()\n","\n","word_counts = Counter()\n","\n","for row in tqdm(dataframe.iterrows(), total=len(dataframe)):\n","    text = row[1].iat[0]\n","\n","    for word in word_tokenize(text):\n","        word_lowercase = word.lower()\n","\n","        if word_lowercase not in stopwords_en:\n","            stemmed_word = porter.stem(word_lowercase)\n","            word_counts[stemmed_word] += 1\n","\n","len(word_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_counts.most_common(10)"]},{"cell_type":"markdown","metadata":{},"source":["## Lematization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Usemos Lemmatization:\n","from nltk import word_tokenize\n","from nltk import pos_tag\n","from nltk.stem import WordNetLemmatizer\n","\n","\n","wnl = WordNetLemmatizer()\n","\n","def penn2morphy(penntag):\n","    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n","    morphy_tag = {'NN':'n', 'JJ':'a',\n","                  'VB':'v', 'RB':'r'}\n","    try:\n","        return morphy_tag[penntag[:2]]\n","    except:\n","        return 'n'\n","\n","word_counts = Counter()\n","\n","for row in tqdm(dataframe.iterrows(), total=len(dataframe)):\n","    text = row[1].iat[0]\n","\n","    for word, tag in pos_tag(word_tokenize(text)):\n","        word_lemmatized = wnl.lemmatize(word.lower(), pos=penn2morphy(tag))\n","\n","        if word_lemmatized not in stopwords_en:\n","            word_counts[word_lemmatized] += 1\n","\n","len(word_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_counts.most_common(10)"]},{"cell_type":"markdown","metadata":{},"source":["# Entrenamientos por clasificador"]},{"cell_type":"markdown","metadata":{},"source":["### Decision Tree"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","dtc = DecisionTreeClassifier()\n","test(dtc)"]},{"cell_type":"markdown","metadata":{},"source":["### MultinomialBN"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.naive_bayes import MultinomialNB\n","mnb = MultinomialNB()\n","test(mnb)"]},{"cell_type":"markdown","metadata":{},"source":["### BernoulliNB"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.naive_bayes import BernoulliNB\n","bnb = BernoulliNB()\n","test(bnb)"]},{"cell_type":"markdown","metadata":{},"source":["### Logistic Regression"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","# lgr = LogisticRegression()\n","lgr = LogisticRegression(max_iter=1000)\n","test(lgr)"]},{"cell_type":"markdown","metadata":{},"source":["### Gaussian"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.naive_bayes import GaussianNB\n","gnb = GaussianNB()\n","test(gnb)"]},{"cell_type":"markdown","metadata":{},"source":["### Random Forest Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","clf = RandomForestClassifier(max_depth=2, random_state=42)\n","test(clf)"]}],"metadata":{"colab":{"provenance":[{"file_id":"1Uo1bHOz9RJfRzCG9S_8o3b1egLTtfyIW","timestamp":1714694532835},{"file_id":"14rAtWC1QzcGMRBgdRjrogYipZ2_RwVbQ","timestamp":1714693907215},{"file_id":"1bdtNIkY_2x9V_MbWLOtI0Qc-Jqm0bBwe","timestamp":1714222939750},{"file_id":"1CZK1XIyCXl3c51Uo3uy50EQqfnAFY1e6","timestamp":1714079362679}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"}},"nbformat":4,"nbformat_minor":0}
